{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:24px; color:red; text-decoration:underline; font-weight:bold;\">\n",
    "  Import\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import monai\n",
    "from monai.data import create_test_image_2d, list_data_collate, decollate_batch, DataLoader\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    EnsureChannelFirstd,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandRotate90d,\n",
    "    ScaleIntensityd,\n",
    ")\n",
    "from monai.visualize import plot_2d_or_3d_image\n",
    "\n",
    "from transform import TrainTransforms,ValidTransforms\n",
    "from loaders import LotusDataModule\n",
    "import argparse\n",
    "import glob \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import UNetLightningModule\n",
    "import argparse\n",
    "\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import NeptuneLogger\n",
    "from pytorch_lightning.strategies.ddp import DDPStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:24px; color:red; text-decoration:underline; font-weight:bold;\">\n",
    "  Test Loader\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type     | Params\n",
      "-------------------------------------------\n",
      "0 | model         | UNet     | 1.6 M \n",
      "1 | loss_function | DiceLoss | 0     \n",
      "-------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 M     Total params\n",
      "6.501     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]data_dict : data_dict :  data_dict : {'img': '/home/lucia/Documents/Gaelle/Data/MultimodelReg/Segmentation/a1_MRI_2D/valid/B020_MR_Crop_slice_040.png', 'seg': '/home/lucia/Documents/Gaelle/Data/MultimodelReg/Segmentation/a2_mri_Seg2D_label2/valid/B020_MR_Seg_clean_slice_040.png'}  {'img': '/home/lucia/Documents/Gaelle/Data/MultimodelReg/Segmentation/a1_MRI_2D/valid/B015_MR_Crop_slice_071.png', 'seg': '/home/lucia/Documents/Gaelle/Data/MultimodelReg/Segmentation/a2_mri_Seg2D_label2/valid/B015_MR_Seg_clean_slice_071.png'}{'img': '/home/lucia/Documents/Gaelle/Data/MultimodelReg/Segmentation/a1_MRI_2D/valid/B022_MR_Crop_slice_013.png', 'seg': '/home/lucia/Documents/Gaelle/Data/MultimodelReg/Segmentation/a2_mri_Seg2D_label2/valid/B022_MR_Seg_clean_slice_013.png'}\n",
      "\n",
      "\n",
      "data_dict : data_dict : data_dict :    data_dict : {'img': '/home/lucia/Documents/Gaelle/Data/MultimodelReg/Segmentation/a1_MRI_2D/valid/B020_MR_Crop_slice_057.png', 'seg': '/home/lucia/Documents/Gaelle/Data/MultimodelReg/Segmentation/a2_mri_Seg2D_label2/valid/B020_MR_Seg_clean_slice_057.png'} \n",
      "{'img': metatensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'seg': metatensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}{'img': metatensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 3., 5.,  ..., 0., 0., 0.],\n",
      "        [1., 3., 5.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'seg': metatensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}\n",
      "{'img': metatensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'seg': metatensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}\n",
      "\n",
      "Before transform: {'img': metatensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'seg': metatensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}Before transform: {'img': metatensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 3., 5.,  ..., 0., 0., 0.],\n",
      "        [1., 3., 5.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'seg': metatensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}\n",
      "\n",
      "Before transform: {'img': metatensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'seg': metatensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}\n",
      "Erreur lors de l'application de la transformation:Erreur lors de l'application de la transformation:data_dict :   Erreur lors de l'application de la transformation:applying transform <monai.transforms.utility.dictionary.EnsureChannelFirstd object at 0x74fe0d4daa30> applying transform <monai.transforms.utility.dictionary.EnsureChannelFirstd object at 0x74fe0d4daa30>\n",
      " \n",
      "Dictionnaire d'entrée :applying transform <monai.transforms.utility.dictionary.EnsureChannelFirstd object at 0x74fe0d4daa30>Dictionnaire d'entrée : \n",
      " Dictionnaire d'entrée : {'img': metatensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'seg': metatensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}\n",
      "{'img': metatensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 3., 5.,  ..., 0., 0., 0.],\n",
      "        [1., 3., 5.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'seg': metatensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}{'img': metatensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'seg': metatensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}{'img': metatensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'seg': metatensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}\n",
      "\n",
      "\n",
      "Before transform: {'img': metatensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'seg': metatensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}\n",
      "data_dict : data_dict :  {'img': '/home/lucia/Documents/Gaelle/Data/MultimodelReg/Segmentation/a1_MRI_2D/valid/B018_MR_Crop_slice_047.png', 'seg': '/home/lucia/Documents/Gaelle/Data/MultimodelReg/Segmentation/a2_mri_Seg2D_label2/valid/B018_MR_Seg_clean_slice_047.png'} data_dict : \n",
      "{'img': '/home/lucia/Documents/Gaelle/Data/MultimodelReg/Segmentation/a1_MRI_2D/valid/B015_MR_Crop_slice_030.png', 'seg': '/home/lucia/Documents/Gaelle/Data/MultimodelReg/Segmentation/a2_mri_Seg2D_label2/valid/B015_MR_Seg_clean_slice_030.png'} \n",
      "{'img': '/home/lucia/Documents/Gaelle/Data/MultimodelReg/Segmentation/a1_MRI_2D/valid/B015_MR_Crop_slice_037.png', 'seg': '/home/lucia/Documents/Gaelle/Data/MultimodelReg/Segmentation/a2_mri_Seg2D_label2/valid/B015_MR_Seg_clean_slice_037.png'}"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 75, in _wrap\n    fn(i, *args)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 580, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 987, in _run\n    results = self._run_stage()\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1031, in _run_stage\n    self._run_sanity_check()\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1060, in _run_sanity_check\n    val_loop.run()\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py\", line 182, in _decorator\n    return loop_run(self, *args, **kwargs)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 128, in run\n    batch, batch_idx, dataloader_idx = next(data_fetcher)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/loops/fetchers.py\", line 133, in __next__\n    batch = super().__next__()\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/loops/fetchers.py\", line 60, in __next__\n    batch = next(self.iterator)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/utilities/combined_loader.py\", line 341, in __next__\n    out = next(self._iterator)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/utilities/combined_loader.py\", line 142, in __next__\n    out = next(self.iterators[0])\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n    data = self._next_data()\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1346, in _next_data\n    return self._process_data(data)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1372, in _process_data\n    data.reraise()\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/torch/_utils.py\", line 705, in reraise\n    raise exception\nRuntimeError: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/transforms/transform.py\", line 141, in apply_transform\n    return _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/transforms/transform.py\", line 98, in _apply_transform\n    return transform(data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(data)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/transforms/utility/dictionary.py\", line 266, in __call__\n    d[key] = self.adjuster(d[key], meta_dict)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/transforms/utility/array.py\", line 227, in __call__\n    result = moveaxis(img, int(channel_dim), 0)  # type: ignore\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/transforms/utils_pytorch_numpy_unification.py\", line 69, in moveaxis\n    return torch.movedim(x, src, dst)  # type: ignore\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/data/meta_tensor.py\", line 282, in __torch_function__\n    ret = super().__torch_function__(func, types, args, kwargs)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/torch/_tensor.py\", line 1443, in __torch_function__\n    ret = func(*args, **kwargs)\nIndexError: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/transforms/transform.py\", line 141, in apply_transform\n    return _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/transforms/transform.py\", line 98, in _apply_transform\n    return transform(data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(data)\n  File \"/home/lucia/Documents/Gaelle/MultimodelRegistration/monai_2d/neptune/transform.py\", line 65, in __call__\n    raise e\n  File \"/home/lucia/Documents/Gaelle/MultimodelRegistration/monai_2d/neptune/transform.py\", line 58, in __call__\n    y = self.train_transform(inp)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/transforms/compose.py\", line 335, in __call__\n    result = execute_compose(\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/transforms/compose.py\", line 111, in execute_compose\n    data = apply_transform(\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/transforms/transform.py\", line 171, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.utility.dictionary.EnsureChannelFirstd object at 0x74fe0d4daa30>\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/data/dataset.py\", line 112, in __getitem__\n    return self._transform(index)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/data/dataset.py\", line 98, in _transform\n    return apply_transform(self.transform, data_i) if self.transform is not None else data_i\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/transforms/transform.py\", line 171, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <transform.ValidTransforms object at 0x74fe0d4daa00>\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 56\u001b[0m\n\u001b[1;32m     42\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     43\u001b[0m     logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     44\u001b[0m     log_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     reload_dataloaders_every_n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# trainer.fit(model, datamodule=concat_dat\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:144\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m process_context \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mstart_processes(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapping_function,\n\u001b[1;32m    138\u001b[0m     args\u001b[38;5;241m=\u001b[39mprocess_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     join\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# we will join ourselves to get the process references\u001b[39;00m\n\u001b[1;32m    142\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocs \u001b[38;5;241m=\u001b[39m process_context\u001b[38;5;241m.\u001b[39mprocesses\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mprocess_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    147\u001b[0m worker_output \u001b[38;5;241m=\u001b[39m return_queue\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:188\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    186\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-- Process \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m error_index\n\u001b[1;32m    187\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m original_trace\n\u001b[0;32m--> 188\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[38;5;241m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 75, in _wrap\n    fn(i, *args)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 580, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 987, in _run\n    results = self._run_stage()\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1031, in _run_stage\n    self._run_sanity_check()\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1060, in _run_sanity_check\n    val_loop.run()\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py\", line 182, in _decorator\n    return loop_run(self, *args, **kwargs)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 128, in run\n    batch, batch_idx, dataloader_idx = next(data_fetcher)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/loops/fetchers.py\", line 133, in __next__\n    batch = super().__next__()\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/loops/fetchers.py\", line 60, in __next__\n    batch = next(self.iterator)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/utilities/combined_loader.py\", line 341, in __next__\n    out = next(self._iterator)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/pytorch_lightning/utilities/combined_loader.py\", line 142, in __next__\n    out = next(self.iterators[0])\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n    data = self._next_data()\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1346, in _next_data\n    return self._process_data(data)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1372, in _process_data\n    data.reraise()\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/torch/_utils.py\", line 705, in reraise\n    raise exception\nRuntimeError: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/transforms/transform.py\", line 141, in apply_transform\n    return _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/transforms/transform.py\", line 98, in _apply_transform\n    return transform(data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(data)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/transforms/utility/dictionary.py\", line 266, in __call__\n    d[key] = self.adjuster(d[key], meta_dict)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/transforms/utility/array.py\", line 227, in __call__\n    result = moveaxis(img, int(channel_dim), 0)  # type: ignore\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/transforms/utils_pytorch_numpy_unification.py\", line 69, in moveaxis\n    return torch.movedim(x, src, dst)  # type: ignore\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/data/meta_tensor.py\", line 282, in __torch_function__\n    ret = super().__torch_function__(func, types, args, kwargs)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/torch/_tensor.py\", line 1443, in __torch_function__\n    ret = func(*args, **kwargs)\nIndexError: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/transforms/transform.py\", line 141, in apply_transform\n    return _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/transforms/transform.py\", line 98, in _apply_transform\n    return transform(data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(data)\n  File \"/home/lucia/Documents/Gaelle/MultimodelRegistration/monai_2d/neptune/transform.py\", line 65, in __call__\n    raise e\n  File \"/home/lucia/Documents/Gaelle/MultimodelRegistration/monai_2d/neptune/transform.py\", line 58, in __call__\n    y = self.train_transform(inp)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/transforms/compose.py\", line 335, in __call__\n    result = execute_compose(\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/transforms/compose.py\", line 111, in execute_compose\n    data = apply_transform(\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/transforms/transform.py\", line 171, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.utility.dictionary.EnsureChannelFirstd object at 0x74fe0d4daa30>\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/data/dataset.py\", line 112, in __getitem__\n    return self._transform(index)\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/data/dataset.py\", line 98, in _transform\n    return apply_transform(self.transform, data_i) if self.transform is not None else data_i\n  File \"/home/lucia/APP/miniconda3/envs/training_mri_cbct2/lib/python3.9/site-packages/monai/transforms/transform.py\", line 171, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <transform.ValidTransforms object at 0x74fe0d4daa00>\n\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_csv = \"/home/lucia/Documents/Gaelle/Data/MultimodelReg/Segmentation/training/label2/train.csv\"\n",
    "val_csv = \"/home/lucia/Documents/Gaelle/Data/MultimodelReg/Segmentation/training/label2/valid.csv\"\n",
    "test_csv = \"/home/lucia/Documents/Gaelle/Data/MultimodelReg/Segmentation/training/label2/test.csv\"\n",
    "batch_size = 2\n",
    "num_workers = 4\n",
    "img_colums = \"mri\"\n",
    "seg_column = \"seg\"\n",
    "# define transforms for image and segmentation\n",
    "train_transforms = TrainTransforms()\n",
    "val_transforms = ValidTransforms()\n",
    "# val_transforms =None\n",
    "\n",
    "loader = LotusDataModule(train_csv,val_csv,test_csv,batch_size,num_workers,img_colums,seg_column,train_transforms,val_transforms)\n",
    "loader.setup()\n",
    "\n",
    "train_ds = loader.train_dataset()\n",
    "val_ds = loader.val_dataset()\n",
    "\n",
    "val_loader = loader.val_dataloader()\n",
    "train_loader = loader.train_dataloader()\n",
    "\n",
    "# print(\"train_ds : \",train_ds)\n",
    "# # print(train_loader)\n",
    "# check_data = monai.utils.misc.first(val_loader)\n",
    "# print(check_data[\"img\"].shape, check_data[\"seg\"].shape)\n",
    "\n",
    "# img = check_data[\"img\"][0][0]  # Assuming single channel and first sample in batch\n",
    "# seg = check_data[\"seg\"][0][0]\n",
    "# plt.figure(\"Check Image\", (12, 6))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.title(\"Image\")\n",
    "# plt.imshow(img.numpy(), cmap=\"gray\")\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.title(\"Segmentation\")\n",
    "# plt.imshow(seg.numpy(), cmap=\"gray\")\n",
    "# plt.show()\n",
    "\n",
    "model = UNetLightningModule()\n",
    "\n",
    "trainer = Trainer(\n",
    "    logger=None,\n",
    "    log_every_n_steps=100,\n",
    "    max_epochs=300,\n",
    "    max_steps=-1,\n",
    "    callbacks=None,\n",
    "    accelerator='gpu', \n",
    "    devices=torch.cuda.device_count(),\n",
    "    strategy=\"ddp_notebook\",\n",
    "    reload_dataloaders_every_n_epochs=1\n",
    ")\n",
    "\n",
    "\n",
    "# trainer.fit(model, datamodule=concat_dat\n",
    "trainer.fit(model, datamodule=loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training_mri_cbct2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
